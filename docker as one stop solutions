Major Previous Shifts in IT:

1. Mainframe-to-PC wave where we shifted from a traditional mainframe architecture that had been around for many, many years to a PC distributed architecture where we're putting in Mac and Windows and DOS. We were changing out our networks and putting in fiber Ethernet and Ethernet 10base-t and TCP/IP.
2. Baremetal-to-Virtual where In the data center and in our servers, we were shifting to virtualization because servers were too powerful, and had a lot of idle time, and we needed to get better utilization. So, we started creating a lot of OS's inside a single piece of hardware. That was virtualization. VMware, Hyper-V and all that stuff.
3. Datacentre-to-Cloud where we had around easy, cheap, disposable compute power that we could spin up and shut down really quickly from a website. It was never that easy and on that level of scale.
4. Host-to-Container where containers would also include serverless, or functions-as-a-service if you've heard of that technology because all those things were made possible by containers and run inside containers to do their work.

Problem in previous technologies:

they were really just built purely for sysadmins, not so much for developers but this solution is an inclusive solution that includes developers, build engineers, testers, operators, sysadmins and everyone in between.

Why do you need Docker? What is the REAL BENEFIT?

It's is all about speed. In fact, all these major shifts in infrastructure have always been about speed. About the speed of software deployments, the speed of business and getting things done in a company for profit. Big benefits in areas like:
1. Develop Faster
2. Build Faster
3. Test Faster
4. Deploy Faster
5. Update Faster
6. Recover Faster as you adopt the various tools of the ecosystem.
But it really covers the entire life cycle of software management

If you don't have containers today, you're probably dealing with something like this. You have multiple types of applications: 
1. Static Website
2. Web Frontend
3. Background Workers
4. User DB
5. Analytics DB
6. Queue

frontend, backend, workers, middle tier. You've got a lot of different things that all need to work together to run your software. Those all will have their own dependencies, 
their own requirements. They might even run on different operating systems or different clouds. Then you have them running on:
1. Desktop (developer machines)
2. Test /QA testing
3. Production Cluster
4. public cloud
5. Data centre
6. Mainframe
7. Windows server
8. Edge devices

the ways of dealing with that is that containers are consistent across the board. It allows packaging the same way regardless of OS. It allows to distribution of the software 
regardless of the setup. It allows you to run and test the software the same way everywhere like Mac, Windows, PC, cloud, data center, edge devices. They all run the same way 
fundamentally when they're using Docker, and it makes a lot of the pain problems around the software life cycle much easier to deal with. The reason that we care so much 
about this is that if you've been here long enough, you know that we have created tons and tons of software in the last 50 years. All the software that we've developed on 
mainframes, which is a lot of it's still around, PCs and now on in the Cloud. All this stuff requires maintenance. We're now saying that 80% of the time of a typical IT 
person is spent managing that existing software. Keeping it updated. Keeping it running. Fixing its problems. Backing it up. All those various things of existing software. 
This leaves very little time left for us to make new things, right, and to deploy new software. So, Docker is freeing up a lot of those tasks of the maintenance of stuff and 
allowing us to get more of our time back to innovate.
